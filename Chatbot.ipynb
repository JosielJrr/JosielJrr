{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN90Ly43w88lJobrOEjTZu3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosielJrr/JosielJrr/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax5t62sEvv1c",
        "outputId": "a01fd0bf-094f-453d-b023-3637e225ee29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.14.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# Instala o pacote da API de gera√ß√£o de texto/imagem da Google (Gemini)\n",
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # M√≥dulo padr√£o pra lidar com vari√°veis de ambiente e sistema\n",
        "from google.colab import userdata  # Acessa dados salvos no ambiente do Colab (ex: chaves)\n",
        "\n",
        "# Pega a chave salva no Colab e define como vari√°vel de ambiente\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "yb5hGRPfzBdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai  # Importa o m√≥dulo principal do SDK Google GenAI\n",
        "\n",
        "# Cria uma inst√¢ncia da classe Client pra acessar os modelos (ex: Gemini)\n",
        "cliente = genai.Client()"
      ],
      "metadata": {
        "id": "taEgKORV0A-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista os modelos dispon√≠veis na conta e imprime o nome de cada um\n",
        "for model in cliente.models.list():\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wahBtChw2HQq",
        "outputId": "35cc5d82-6c65-489a-9a5b-167a96e1cc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = 'gemini-2.0-flash'  # Define o nome do modelo que ser√° usado\n",
        "\n",
        "# Envia uma pergunta pro modelo escolhido e armazena a resposta\n",
        "resposta = cliente.models.generate_content(\n",
        "    model=modelo,\n",
        "    contents='Quando a Google lan√ßou o Gemini?'\n",
        ")\n",
        "\n",
        "# Imprime o texto gerado pelo modelo na resposta\n",
        "print(resposta.text)"
      ],
      "metadata": {
        "id": "kUb_QQnP2cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0760c8c6-e164-42dd-f96d-2bd12ed365da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O Google lan√ßou o Gemini em fases:\n",
            "\n",
            "*   **6 de dezembro de 2023:** An√∫ncio oficial do Gemini e lan√ßamento da vers√£o \"Gemini Pro\" para desenvolvedores e empresas atrav√©s da Google AI Studio e Google Cloud Vertex AI.\n",
            "*   **7 de dezembro de 2023:** Integra√ß√£o do Gemini Pro ao Bard (em ingl√™s).\n",
            "*   **Fevereiro de 2024:** Lan√ßamento do \"Gemini 1.5 Pro\" para um grupo seleto de usu√°rios para testes.\n",
            "*   **Fevereiro de 2024:** Lan√ßamento do \"Gemini Nano\" no Pixel 8 Pro.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma sess√£o de chat com o modelo escolhido\n",
        "chat = cliente.chats.create(model=modelo)\n",
        "\n",
        "# Envia uma mensagem no chat e recebe a resposta\n",
        "resposta = chat.send_message('Oi, tudo bem?')\n",
        "\n",
        "# Imprime a resposta do modelo\n",
        "print(resposta.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1kP-aKD5P4o",
        "outputId": "478b601e-cb3b-4a88-ca7c-3f8d76eea46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tudo bem por aqui! üòä Como posso te ajudar hoje?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Envia uma mensagem com uma instru√ß√£o de comportamento (system instruction embutida) e faz a pergunta\n",
        "resposta = chat.send_message(\n",
        "    'Voc√™ √© um assistente pessoal que responde sempre de forma sucinta: O que √© Intelig√™ncia Artificial?'\n",
        ")\n",
        "\n",
        "# Imprime a resposta gerada com base no estilo solicitado\n",
        "print(resposta.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ap_y-5L6ebH",
        "outputId": "ab26bc07-0cee-4f2c-b33a-fef0fccf2313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Habilidade de m√°quinas emular intelig√™ncia humana.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types  # Importa os tipos usados pra configurar a gera√ß√£o de conte√∫do\n",
        "\n",
        "# Cria uma configura√ß√£o com system instruction, definindo o comportamento do modelo\n",
        "chat_config = types.GenerateContentConfig(\n",
        "    system_instruction='Voc√™ √© um assistente pessoal que responde sempre de forma sucinta.',\n",
        ")\n",
        "\n",
        "# Cria uma sess√£o de chat com o modelo e aplica a configura√ß√£o definida\n",
        "chat = cliente.chats.create(model=modelo, config=chat_config)"
      ],
      "metadata": {
        "id": "ruOEk8Bw76MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pergunta pro modelo (a resposta j√° vem sucinta por causa da system instruction)\n",
        "resposta = chat.send_message('O que √© computa√ß√£o qu√¢ntica?')\n",
        "\n",
        "# Mostra a resposta do modelo\n",
        "print(resposta.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6JgQtEs9Amg",
        "outputId": "127e3ea5-90b3-4461-c126-40a1c56c4f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√â um tipo de computa√ß√£o que usa fen√¥menos da mec√¢nica qu√¢ntica, como superposi√ß√£o e entrela√ßamento, para resolver problemas complexos que computadores cl√°ssicos n√£o conseguem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega o hist√≥rico de mensagens trocadas na sess√£o de chat atual\n",
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgx1R8xh9w8P",
        "outputId": "6e7264c0-45d4-46b0-fa76-5753132ce6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='O que √© computa√ß√£o qu√¢ntica?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='√â um tipo de computa√ß√£o que usa fen√¥menos da mec√¢nica qu√¢ntica, como superposi√ß√£o e entrela√ßamento, para resolver problemas complexos que computadores cl√°ssicos n√£o conseguem.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recebe a pergunta do usu√°rio\n",
        "prompt = input('Envie sua pergunta: ')\n",
        "\n",
        "# Loop que mant√©m o chat ativo at√© o usu√°rio digitar 'sair'\n",
        "while prompt != 'sair':\n",
        "    resposta = chat.send_message(prompt)  # Envia a pergunta pro modelo\n",
        "    print(f'Resposta: {resposta.text}\\n')  # Mostra a resposta\n",
        "\n",
        "    # Pede a pr√≥xima pergunta ou sair\n",
        "    prompt = input('Digite sua pr√≥xima pergunta ou \"sair\" para encerrar: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YghlEXsF-KmY",
        "outputId": "4ef8b247-34c4-4969-e5b3-7c24cbeb2029"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Envie sua pergunta: O que √© a Alura?\n",
            "Resposta: √â uma plataforma de cursos online focada em tecnologia e desenvolvimento de software.\n",
            "\n",
            "\n",
            "Digite sua pr√≥xima pergunta ou \"sair\" para encerrar: O que √© o Google Gemini?\n",
            "Resposta: √â um modelo de intelig√™ncia artificial multimodal do Google.\n",
            "\n",
            "\n",
            "Digite sua pr√≥xima pergunta ou \"sair\" para encerrar: sair\n"
          ]
        }
      ]
    }
  ]
}